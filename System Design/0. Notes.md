System Design: A Comprehensive Guide
System design involves defining the architecture, components, and data flow of a system to meet functional and non-functional requirements. It ensures scalability, reliability, maintainability, and performance. Below are key concepts in system design:

1. Scalability

Scalability refers to a system's ability to handle increased load efficiently.
* Horizontal Scaling (Scaling Out)
    * Adding more machines (servers) to distribute the load.
    * Example: Instead of upgrading a single database, distribute data across multiple databases (sharding).
* Vertical Scaling (Scaling Up)
    * Increasing the resources (CPU, RAM, Disk) of an existing machine.
    * Example: Upgrading a server from 8GB RAM to 32GB RAM.
Comparison:
* Horizontal Scaling is more flexible and widely used in cloud environments.
* Vertical Scaling has hardware limits and downtime during upgrades.




2. Load Balancing

Load balancing ensures that incoming network traffic is distributed across multiple servers to prevent overloading any single server.
* Popular Load Balancers:
    * Nginx, HAProxy, AWS Elastic Load Balancer (ELB).

=> Roles of Load Balancer :-

* Load distribution is equal over every node.
* Health check - if a server is not healthy will not send further requests to it.
* ensures high scalibility, High throughput and high availability(Horizontal scaling)

**When to use it and when not to ?**

* In monolithic or in case of vertically scaled system we don't need it. But in microservice architecture we need

**Challenges of Load Balancing**

* Single Point Of Failure - During a load balancer malfunctioning, the communication between clients and servers would be broken. 
    To solve this issue, we can use redundancy. The system can have an active load balancer and one passive load balancer

**Advantages of using load balancers:**
1. *Optimisation*: Load Balancers help in resource utilisation and lower response time, thereby optimising the system in a high traffic environment.
2. *Better User Experience:* Load balancers help in reducing the latency and increasing availability, making the user's request go smoothly and error-free.
3. *Prevents Downtime:* Load Balancers maintain a record of servers that are non-operational and distribute the traffic accordingly, therefore ensuring security and preventing downtime, which also helps increase profits and productivity.
4. *Flexibility*: Load Balancers have the flexibility to re-route the traffic in case of any breakdown and work on server maintenance to ensure efficiency. This helps in avoiding traffic bottlenecks.
5. *Scalability*: When the traffic of a web application increases suddenly, load balancers can use physical or virtual servers to deliver the responses without any disruption.
6. *Redundancy*: Load Balancing also provides in-build redundancy by re-routing the traffic in case of any

**Load Balancing Algorithms**
1. Round Robin (Static) - rotation fashion.
2. Weighted Roud robin (Static) - It is similar to the Round Robin when the servers are of different capacities. (some node can have better resoures, others might not have)
3. IP Hash Algorithm (Static) - The servers have almost equal capacity, and the hash function ( input is source IP) is used for random or unbiased distribution of requests to the nodes.
4. Source IP Hash (Static) - Source IP Hash combines the server and client's source and destination
IP addresses to produce a hash key. The key can be used to determine the request distribution.
5. Least Connection Algorithm (Dynamic) - Client requests are distributed to the application server with the least number of active connections at the time the client request is received.
6. Least Response Time (Dynamic) - The request is distributed based on the server which has the least response




3. Caching

Caching speeds up data retrieval by storing frequently accessed data in memory.
* Types of Caching:
    * Client-side Cache: Stored on the user's browser (e.g., cookies, local storage).
    * Server-side Cache: Stored on backend systems (e.g., Redis, Memcached).
    * Content Delivery Network (CDN): Globally distributed cache for static content (e.g., images, videos).
* Example:
    * Instead of querying a database for the same information repeatedly, store it in Redis for quick access.

1. Why Use Caching?
* Reduces Latency → Fetching data from memory is much faster than querying a database.
* Improves Performance → Offloads requests from the database, reducing load.
* Saves Costs → Fewer database queries mean less computational cost.
* Enhances Scalability → Helps handle high traffic efficiently.

2. Types of Caching
    1. Client-Side Caching
        * Data is cached on the user’s device (browser, app)
        * Example: Browser caching (HTML, CSS, JS, images)
        * Use Case: Reducing repeated downloads from the server.
    2. Server-Side Caching
        * Cache is stored at the server level for quick access.
        * Examples:
            * Application-level cache (In-memory storage like Redis or Memcached)
            * Database query caching (Storing frequently used query results)
3. Content Delivery Network (CDN) Caching
    * CDNs cache static content across multiple locations worldwide.
    * Example: Cloudflare, Akamai caching images, videos, and scripts.
    * Use Case: Reduces load on the main server and speeds up content delivery.

3. Caching Strategies
1. Write-Through Cache
* Data is written to both cache and database simultaneously.
* Ensures consistency but increases write latency.
* Use Case: Good for systems where data consistency is critical.
2. Write-Back Cache
* Data is written to cache first and later written to the database asynchronously.
* Faster writes but risk of data loss if the cache fails before syncing.
3. Cache-aside (Lazy Loading)
* Data is loaded into the cache only when requested.
* If not in cache, fetch from DB, store in cache, and return result.
* Use Case: Balances performance and freshness of data.
4. Time-to-Live (TTL) & Expiry Policies
* Cached data expires after a set time to avoid stale data.
* Example: Setting a TTL of 5 minutes for API responses.

4. When to Use Caching?
✅ High Read Operations: Repeatedly accessed data (e.g., product catalogs, user profiles).
✅ Expensive Queries: Queries that require complex joins or aggregations.
✅ High Traffic Systems: Caching helps handle millions of requests per second (e.g., social media feeds, trending searches).

5. Caching in Your Cloud Storage Project
    * Metadata Caching: Store file metadata (file name, size, owner) in Redis to reduce DB queries.
    * Session Caching: Store active user sessions in Memcached for faster authentication.
    * File Previews: Cache frequently accessed documents or images using a CDN.
    * Rate Limiting: Use Redis to prevent abuse of the API by caching request counts.





4. Database Design

Efficient database design ensures data integrity and performance.
* Normalization:
    * Reduces redundancy by organizing data into separate tables.
    * Example: A "Users" table and a separate "Orders" table, rather than storing order details in the same user record.
* Denormalization:
    * Combines tables to improve read performance.
    * Example: Storing order details directly in the "Users" table for faster access.
* Sharding:
    * Splits large databases into smaller, distributed ones.
    * Example: Instead of one database handling 10 million users, divide it into 10 databases of 1 million users each.




5. CAP Theorem (Consistency, Availability, Partition Tolerance)

    The CAP theorem states that a distributed system can achieve only two of the three properties:
        * Consistency (C): All nodes see the same data at the same time.
        * Availability (A): The system remains operational at all times.
        * Partition Tolerance (P): The system continues working even if some nodes fail.
        * Trade-offs:
            * CA System: Traditional databases like MySQL (Consistency + Availability, but fails under network partition).
            * AP System: NoSQL databases like DynamoDB (Availability + Partition Tolerance, but may have stale reads).
            * CP System: Databases like MongoDB (Consistency + Partition Tolerance, but may become temporarily unavailable).




6. Microservices

    Microservices architecture divides an application into small, independent services that communicate via APIs.
    * Advantages:
        * Easier to scale individual components.
        * Improves fault isolation (if one service fails, others remain unaffected).
        * Faster development and deployment.
    * Example:
        * An e-commerce system may have separate microservices for User Management, Orders, Payments, and Inventory.




7. Message Queues

    Message queues allow asynchronous communication between services.
    * Why use message queues?
        * Decouples systems to avoid dependencies.
        * Improves performance by handling background tasks efficiently.
    * Examples of Message Queues:
        * RabbitMQ: Uses a publish-subscribe model.
        * Kafka: A distributed event streaming platform for high-throughput applications.




8. API Design

APIs allow communication between different parts of a system.
* Types of APIs:
    * RESTful APIs:
        * Stateless, follows HTTP methods like GET, POST, PUT, DELETE.
        * Example: A GET /users/{id} API fetches user data.
    * GraphQL:
        * Clients can request specific fields instead of fetching unnecessary data.
        * Example: Instead of GET /users, GraphQL allows querying { user(id: 1) { name, email } }.




9. Security
Security ensures that the system is protected against attacks.
* Authentication vs. Authorization:
    * Authentication: Verifies "Who are you?" (e.g., Login with username & password).
    * Authorization: Verifies "What can you do?" (e.g., Admin vs. Regular User access).
* Encryption:
    * Data at Rest: Encrypts stored data (e.g., AES encryption for databases).
    * Data in Transit: Encrypts communication (e.g., SSL/TLS for HTTPS).




10. Monitoring and Logging
Monitoring tools help track system performance and detect issues.
* Tools:
    * Prometheus, Grafana: Monitor system health.
    * ELK Stack (Elasticsearch, Logstash, Kibana): Collect and analyze logs.




11. Fault Tolerance

Fault tolerance ensures that a system remains operational despite failures.
* Techniques:
    * Redundancy: Keep backup servers ready.
    * Failover Mechanism: If one server fails, another takes over.
    * Circuit Breakers: Prevent repeated failures by stopping problematic requests.




12. CDN (Content Delivery Network)
CDNs store cached copies of content in geographically distributed servers to reduce latency.
* Examples: Cloudflare, Akamai.
* Use case: A website serving global users loads faster by using CDN caching.




13. Data Replication
Data replication creates multiple copies of data to ensure reliability.
* Types:
    * Synchronous Replication: Updates all copies immediately.
    * Asynchronous Replication: Updates copies after a delay.




14. Rate Limiting
Rate limiting controls how many requests a user can make in a given time.
* Use Case: Prevents DDoS attacks and API abuse.
* Example: Allowing only 100 requests per minute per user.




15. Design Patterns
Common system design patterns:
* Singleton: Ensures a class has only one instance.
* Factory Pattern: Creates objects without exposing instantiation logic.
* Observer Pattern: Allows one-to-many dependency (e.g., event listeners).
* Circuit Breaker: Stops calls to a failing service to prevent system overload.




16. Event-Driven Architecture
An event-driven system responds to events rather than direct requests.
* Example: A payment service triggers an Order Processed event, which other services (e.g., shipping) listen to.




17. High Availability

Availability refers to the system's ability to remain operational and accessible without downtime. A highly available system ensures that services continue to function even in the presence of failures.
High availability ensures minimal downtime.

* Fault Tolerence ∝ Availability
* Techniques: Load balancing, redundancy, failover systems.

=> How to increase redundency :-

* Replication - Redundency + Data Sync between Nodes (DB)
* Distributed System
* Redundency (microservices)




18. Edge Computing
Edge computing processes data closer to the source instead of relying on centralized cloud servers.
* Example: IoT devices processing data locally before sending summaries to the cloud.




19. Storage Solutions
Different types of storage for different needs:
* SQL Databases: Structured data (MySQL, PostgreSQL).
* NoSQL Databases: Unstructured data (MongoDB, Cassandra).
* Object Storage: Stores files (Amazon S3, Google Cloud Storage).




20. Networking Basics
Understanding how data moves in a system:
* HTTP/HTTPS: Communication over the web.
* DNS: Resolves domain names to IP addresses.
* TCP/IP: Protocols for reliable data transfer.




21. DevOps Practices
DevOps ensures smooth deployment and system management.
* CI/CD Pipelines: Automate code deployment.
* Infrastructure as Code (IaC): Manage servers using code (Terraform, Ansible).
* Containerization: Package applications for consistent deployment (Docker, Kubernetes).




21. Trade-offs in System Design
Every decision in system design has trade-offs:
* Performance vs. Consistency: Strong consistency may slow down performance.
* Cost vs. Scalability: More scalability usually increases costs.
* Complexity vs. Maintainability: More components can make maintenance harder.


22. Latency

A request goes and a response comes back—this duration is called latency.

Latency = Network delay + computational delay.

* Latency is the total time taken for a request to travel from the client to the server and for the     
    response to return to the client. It consists of multiple components:

1. Network Latency – Time taken for data to travel over the network.
2. Processing Time – Time taken by the server to process the request.
3. Queuing Delay – Time spent waiting in queues if the server is handling multiple requests.
4. Database Latency – Time taken for database queries and data retrieval.
5. Serialization/Deserialization Time – Time taken to encode/decode the request and response.

=> Reducing latency involves:

1. Implementing caching to avoid redundant computations.
2. Optimizing database queries for faster retrieval.
3. Using load balancing to distribute traffic efficiently.
4. Implementing asynchronous processing where applicable.
5. Using CDNs to bring content closer to users.

23. Throughput

* Throughput refers to how much work a system can complete in a given period. It is typically measured in requests per second (RPS), 
    transactions per second (TPS), or data processed per second (e.g., MB/s, GB/s).


=>  Key Factors Affecting Throughput:

1. CPU & Memory Performance – More powerful hardware allows faster processing.
2. Network Bandwidth – A system can only handle as many requests as its network allows.
3. Concurrency & Parallelism – Efficient multi-threading or distributed processing increases throughput.
4. Database Performance – Optimized queries, indexing, and caching improve data retrieval speed.
5. Load Balancing – Distributing tasks across multiple servers prevents overload.
6. Asynchronous Processing – Queues (Kafka, RabbitMQ) decouple tasks and improve efficiency.
7. Bottlenecks – A slow component (e.g., disk I/O, external API calls) can limit overall throughput.


=> Throughput vs. Latency

* High Latency -> Low Throughput
* Latency is the time taken for a single request to complete.
* Throughput is how many requests the system can handle per second.
* A system can have low latency but low throughput (fast but handles few requests) or high throughput but high latency (handles many requests but each takes longer).

=> Causes of Low Throughput

* High Latency
* Protocol Overhead
* Congestion - a large number of request at a same time.

=> Improving Throughput

* Caching
* CDN
* Distributed system
* Load Balancer
* Upgrade resources


24. Consistency

Consistency ensures that all nodes in a distributed system see the same data at the same time. If a system is strongly consistent, any 
    read operation will return the most recent write.


Example of Consistency: Bank Transactions

Imagine you have an online banking system with multiple replicas (servers) handling requests.

Scenario: Transferring Money
* You transfer ₹1000 from Account A to Account B.
* The system updates Account A's balance (₹5000 → ₹4000).
* The system updates Account B's balance (₹3000 → ₹4000).

Strong Consistency (CP System)
* If you refresh the page, you will immediately see the updated balance (₹4000 in Account A and ₹4000 in Account B).
* Any read operation will return the most recent write, even if it means waiting a little longer.
* Example: Traditional SQL databases like PostgreSQL, MySQL (ACID-compliant systems).

Eventual Consistency (AP System)
* If the bank prioritizes high availability, your first balance check might still show ₹5000 in Account A for a short time.
* The system will eventually sync, and after a few seconds or minutes, you will see the correct balance.
* Example: NoSQL databases like Cassandra, DynamoDB (which prioritize availability).




Partition Tolerance (P) in CAP Theorem

Partition - a server may go doing - will be partitioned by other servers
Partition Tolerance - use a replica

Partition Tolerance means that a distributed system must continue to operate even if there is a network failure (partition) that prevents 
 communication between some nodes.
In real-world systems, network failures will happen, so any distributed system must be partition-tolerant to ensure availability and consistency as much as possible.

Example: E-commerce Website (Amazon, Flipkart, etc.)
Scenario: A Network Partition Occurs
Imagine an e-commerce system where multiple servers are handling user requests.
1. A user places an order for a smartphone from Server A.
2. At the same time, another user is also trying to buy the last unit, but their request goes to Server B.
3. Suddenly, the network link between Server A and Server B fails (network partition).

Handling the Partition:

1. If the System Chooses Consistency (CP)
* The system stops processing orders until the partition is resolved.
* This ensures that no two users buy the last unit at the same time.
* Drawback: The system becomes unavailable during the failure.

2. If the System Chooses Availability (AP)
* Both Server A and Server B process the orders independently (allowing both users to buy the last unit).
* Once the partition is resolved, the system detects the conflict and resolves it later (e.g., by canceling one order and refunding).
* Drawback: Temporary inconsistency in the inventory system.

Why Partition Tolerance is Important?
* In a distributed system, network failures are inevitable.
* We cannot sacrifice partition tolerance, because if nodes cannot communicate, they must still function independently.
* That’s why real-world systems choose between CP (Consistency & Partition Tolerance) or AP (Availability & Partition Tolerance) but cannot 
 have both C & A fully at the same time during a partition.




